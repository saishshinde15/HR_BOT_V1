# ============================================================
# AWS Bedrock Configuration (Primary LLM and Embeddings)
# ============================================================
# Get your credentials from AWS Console
# Ensure Bedrock access is enabled in your AWS account
# Required for Amazon Bedrock Nova Pro (production model)
AWS_ACCESS_KEY_ID=your_aws_access_key_here
AWS_SECRET_ACCESS_KEY=your_aws_secret_key_here
AWS_REGION=us-east-1

# Model Configuration - Production Setup
# Nova Pro: $3/1M input tokens, $12/1M output tokens (75% cheaper than GPT-4)
LLM_MODEL=bedrock/amazon.nova-pro-v1:0
EMBEDDING_MODEL=bedrock/amazon.titan-embed-text-v2:0

# Alternative Models (for testing/cost optimization)
# BEDROCK_MODEL=bedrock/us.amazon.nova-micro-v1:0  # Even cheaper, lower quality

# ============================================================
# API Deck Configuration (HR System Integration)
# ============================================================
# Get your credentials from: https://developers.apideck.com
# After signing up, connect your HR platform (SAP, Zoho, BambooHR, etc.)
APIDECK_API_KEY=your_apideck_api_key_here
APIDECK_APP_ID=your_apideck_app_id_here
APIDECK_SERVICE_ID=your_hr_service_id_here

# ============================================================
# Semantic Cache Configuration (Production-Grade Caching)
# ============================================================
# Intelligent caching with semantic similarity matching
# Dramatically improves response times for similar queries
ENABLE_CACHE=true
CACHE_TTL_HOURS=72  # Cache expires after 72 hours (3 days)
CACHE_SIMILARITY_THRESHOLD=0.60  # 60% similarity threshold for cache hits

# Cache Performance:
# - First query: 4-8s (full execution)
# - Cached query: 0.1-0.5s (95% faster!)
# - Expected hit rate: 30-40% in production

# ============================================================
# Hybrid RAG Configuration (BM25 + Vector Search)
# ============================================================
# Document chunking settings
CHUNK_SIZE=800              # Characters per chunk
CHUNK_OVERLAP=200           # Overlap between chunks for context

# Retrieval settings
TOP_K_RESULTS=5             # Number of top results to retrieve

# Hybrid search weights (must sum to 1.0)
BM25_WEIGHT=0.5             # Keyword/lexical search weight
VECTOR_WEIGHT=0.5           # Semantic/embedding search weight

# Performance tuning:
# - Increase BM25_WEIGHT for exact term matching (policy names, dates)
# - Increase VECTOR_WEIGHT for semantic understanding (meaning-based queries)
